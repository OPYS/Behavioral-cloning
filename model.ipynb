{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, ELU\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "import pickle\n",
    "from scipy import misc\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "nb_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [-0.5, 0.5]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    #costants\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    xmin = np.min(image_data)\n",
    "    xmax = np.max(image_data) \n",
    "    \n",
    "    x = image_data    \n",
    "    x_prime = a + ((x-xmin)*(b-a))/(xmax-xmin)\n",
    "    \n",
    "    return x_prime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8036, 66, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "images_list = []\n",
    "\n",
    "for infile in glob.glob(\"IMG/center*.jpg\"):\n",
    "    image = misc.imread(infile)\n",
    "    image = misc.imresize(image, (66,200))\n",
    "    images_list.append(image)\n",
    "    \n",
    "X_train = np.stack(images_list)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8036, 7)\n",
      "(8036,)\n"
     ]
    }
   ],
   "source": [
    "with open('driving_log.csv','r') as file:\n",
    "    datareader = csv.reader(file,delimiter=',')\n",
    "    driving_log = []\n",
    "    for row in datareader:\n",
    "        driving_log.append(row)\n",
    "        \n",
    "driving_log.pop(0)\n",
    "        \n",
    "log = np.stack(driving_log)\n",
    "print(log.shape)\n",
    "\n",
    "#labels\n",
    "y_train = np.zeros(log.shape[0])\n",
    "for i,row in enumerate(log):\n",
    "    y_train[i] = log[i][3]\n",
    "    \n",
    "    \n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8036, 66, 200, 3)\n",
      "Data normalized\n",
      "8036\n"
     ]
    }
   ],
   "source": [
    "x_train_normalized = []\n",
    "for image_data in X_train:\n",
    "    normalized_image = normalize(image_data)\n",
    "    x_train_normalized.append(normalized_image)\n",
    "    \n",
    "\n",
    "print(X_train.shape)\n",
    "print('Data normalized')\n",
    "\n",
    "x_train_normalized = map(normalize, X_train)\n",
    "print (len(list(x_train_normalized)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Lambda(lambda x:x/127.5 -1., input_shape = (66,200,3)))\n",
    "model.add(Convolution2D(24, 5, 5,border_mode='valid',subsample=(2,2), dim_ordering='tf', input_shape=(66,200,3), init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(36, 5, 5, border_mode='valid',subsample=(2,2), init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(48, 5, 5, border_mode='valid',subsample=(2,2),init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid',subsample=(1,1),init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid',subsample=(1,1),init='he_normal'))\n",
    "model.add(Flatten())\n",
    "model.add(ELU())\n",
    "model.add(Dense(1164,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(100,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(50,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(10,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(1,name='output',init='he_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5625 samples, validate on 2411 samples\n",
      "Epoch 1/5\n",
      "5625/5625 [==============================] - 52s - loss: 0.6162 - acc: 0.4891 - val_loss: 0.0186 - val_acc: 0.5367\n",
      "Epoch 2/5\n",
      "5625/5625 [==============================] - 51s - loss: 0.0180 - acc: 0.5452 - val_loss: 0.0188 - val_acc: 0.5367\n",
      "Epoch 3/5\n",
      "5625/5625 [==============================] - 50s - loss: 0.0174 - acc: 0.5452 - val_loss: 0.0202 - val_acc: 0.5367\n",
      "Epoch 4/5\n",
      "5625/5625 [==============================] - 50s - loss: 0.0173 - acc: 0.5452 - val_loss: 0.0184 - val_acc: 0.5367\n",
      "Epoch 5/5\n",
      "5625/5625 [==============================] - 52s - loss: 0.0171 - acc: 0.5452 - val_loss: 0.0171 - val_acc: 0.5367\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.545244444338\n"
     ]
    }
   ],
   "source": [
    "print(history.history['acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "with open('model.json','w') as f:\n",
    "    json.dump(json_string,f,ensure_ascii=False)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
