{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, ELU\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "import pickle\n",
    "from scipy import misc\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "import json\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "nb_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_path = 'driving_log.csv'\n",
    "data_files_s = pd.read_csv(csv_path,\n",
    "                         index_col = False)\n",
    "data_files_s.columns = ['center', 'left', 'right', 'steer', 'throttle', 'brake', 'speed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def crop_image(image):\n",
    "    #New sizes for image, suggested the Vivek in here \n",
    "    #https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.n2mwq1z33\n",
    "    col, row = 200,66\n",
    "    \n",
    "    shape = image.shape\n",
    "    \n",
    "    #Cut off the sky from the original picture\n",
    "    crop_up = shape[0]/5\n",
    "    \n",
    "    #Cut off the front of the car\n",
    "    crop_down = shape[0]-25\n",
    "\n",
    "    image = image[crop_up:crop_down, 0:shape[1]]\n",
    "    image = cv2.resize(image,(col,row), interpolation=cv2.INTER_AREA)    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image_file_train(line_data):\n",
    "    # Preprocessing training files and augmenting\n",
    "    i_lrc = np.random.randint(6)\n",
    "    if (i_lrc == 0):\n",
    "        path_file = line_data['left'][0].strip()\n",
    "        shift_ang = .25\n",
    "    elif (i_lrc == 1):\n",
    "        path_file = line_data['right'][0].strip()\n",
    "        shift_ang = -.25\n",
    "    else: # (i_lrc in range(1,5)):\n",
    "        path_file = line_data['center'][0].strip()\n",
    "        shift_ang = 0\n",
    "        \n",
    "    y_steer = line_data['steer'][0] + shift_ang\n",
    "    image = cv2.imread(path_file)\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image(image)\n",
    "    image = np.array(image)\n",
    "    ind_flip = np.random.randint(2)\n",
    "    if ind_flip==0:\n",
    "        image = cv2.flip(image,1)\n",
    "        y_steer = -y_steer\n",
    "    \n",
    "    return image,y_steer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_size_row = 66\n",
    "new_size_col = 200\n",
    "def generate_train_from_PD_batch(data,batch_size = 32):\n",
    "    ## Generator for keras training, with subsampling  \n",
    "    batch_images = np.zeros((batch_size, new_size_row, new_size_col, 3))\n",
    "    batch_steering = np.zeros(batch_size)\n",
    "    while 1:\n",
    "        for i_batch in range(batch_size):\n",
    "            i_line = np.random.randint(len(data))\n",
    "            line_data = data.iloc[[i_line]].reset_index()\n",
    "            x,y = preprocess_image_file_train(line_data)\n",
    "            \n",
    "            batch_images[i_batch] = x\n",
    "            batch_steering[i_batch] = y\n",
    "            #if x is None:\n",
    "            #    print('Image is None for i_line:', i_line)\n",
    "            #if y is None:\n",
    "            #    print('Steering is None for i_line:', i_line)\n",
    "\n",
    "        \n",
    "        yield batch_images, batch_steering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image_file_predict(line_data):\n",
    "    # Preprocessing Prediction files and augmenting\n",
    "    path_file = line_data['center'][0].strip()\n",
    "    #print(path_file)\n",
    "    image = cv2.imread(path_file)\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image(image)\n",
    "    image = np.array(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_valid_from_PD(data):\n",
    "    # Validation generator\n",
    "    while 1:\n",
    "        for i_line in range(len(data)):\n",
    "            line_data = data.iloc[[i_line]].reset_index()\n",
    "            #print(line_data)\n",
    "            x = preprocess_image_file_predict(data)\n",
    "            x = x.reshape(1, x.shape[0], x.shape[1], x.shape[2])\n",
    "            y = line_data['steer'][0]\n",
    "            y = np.array([[y]])\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(lambda x:x/127.5 -1., input_shape = (66,200,3)))\n",
    "model.add(Convolution2D(24, 5, 5,border_mode='valid',subsample=(2,2), dim_ordering='tf', input_shape=(66,200,3), init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(36, 5, 5, border_mode='valid',subsample=(2,2), init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(48, 5, 5, border_mode='valid',subsample=(2,2),init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid',subsample=(1,1),init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid',subsample=(1,1),init='he_normal'))\n",
    "model.add(Flatten())\n",
    "model.add(ELU())\n",
    "model.add(Dense(1164,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(100,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(50,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(10,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(1,name='output',init='he_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(fileModelJSON,fileWeights):\n",
    "    #print(\"Saving model to disk: \",fileModelJSON,\"and\",fileWeights)\n",
    "    if Path(fileModelJSON).is_file():\n",
    "        os.remove(fileModelJSON)\n",
    "    json_string = model.to_json()\n",
    "    with open(fileModelJSON,'w' ) as f:\n",
    "        json.dump(json_string, f)\n",
    "    if Path(fileWeights).is_file():\n",
    "        os.remove(fileWeights)\n",
    "    model.save_weights(fileWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "#history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josemacenteno/anaconda3/envs/keras/lib/python3.5/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20224/20224 [==============================] - 217s - loss: 0.0263 - acc: 0.3654 - val_loss: 0.0266 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 224s - loss: 0.0210 - acc: 0.3655 - val_loss: 0.0247 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 216s - loss: 0.0194 - acc: 0.3553 - val_loss: 0.0272 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 234s - loss: 0.0190 - acc: 0.3567 - val_loss: 0.0363 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 310s - loss: 0.0176 - acc: 0.3657 - val_loss: 0.0250 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 217s - loss: 0.0174 - acc: 0.3608 - val_loss: 0.0378 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 214s - loss: 0.0172 - acc: 0.3604 - val_loss: 0.0374 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 214s - loss: 0.0173 - acc: 0.3617 - val_loss: 0.0334 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 214s - loss: 0.0162 - acc: 0.3618 - val_loss: 0.0383 - val_acc: 0.5427\n",
      "Epoch 1/1\n",
      "20224/20224 [==============================] - 213s - loss: 0.0157 - acc: 0.3591 - val_loss: 0.0262 - val_acc: 0.5427\n",
      "Best model found at iteration # 1\n",
      "Best Validation score : 0.0247\n"
     ]
    }
   ],
   "source": [
    "valid_s_generator = generate_valid_from_PD(data_files_s)\n",
    "\n",
    "\n",
    "val_size = len(data_files_s)\n",
    "pr_threshold = 1\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "i_best = 0\n",
    "val_best = 1000\n",
    "\n",
    "\n",
    "for i_pr in range(10):\n",
    "\n",
    "    train_r_generator = generate_train_from_PD_batch(data_files_s,batch_size)\n",
    "\n",
    "    nb_vals = np.round(len(data_files_s)/val_size)-1\n",
    "    history = model.fit_generator(train_r_generator,\n",
    "            samples_per_epoch=20224, nb_epoch=1,validation_data=valid_s_generator, nb_val_samples=val_size)\n",
    "    \n",
    "    fileModelJSON = 'model_' + str(i_pr) + '.json'\n",
    "    fileWeights = 'model_' + str(i_pr) + '.h5'\n",
    "    \n",
    "    save_model(fileModelJSON,fileWeights)\n",
    "    \n",
    "    val_loss = history.history['val_loss'][0]\n",
    "    if val_loss < val_best:\n",
    "        i_best = i_pr \n",
    "        val_best = val_loss\n",
    "        fileModelJSON = 'model_best.json'\n",
    "        fileWeights = 'model_best.h5'\n",
    "        save_model(fileModelJSON,fileWeights)\n",
    "    \n",
    "    \n",
    "    pr_threshold = 1/(i_pr+1)\n",
    "    \n",
    "print('Best model found at iteration # ' + str(i_best))\n",
    "print('Best Validation score : ' + str(np.round(val_best,4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
